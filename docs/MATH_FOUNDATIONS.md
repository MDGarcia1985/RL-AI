Mathematical Foundations for Reinforcement Learning (RC Agents)

This document explains the core math required to understand how this project works.

## Table of Contents
- [Table of Contents](#table-of-contents)
- [

1. Numbers and Precision
Integers (int)

Whole numbers:

0, 1, 2, 10, -3

Used for:

Grid coordinates

Episode counters

Action IDs

Floating Point Numbers (float)

Decimals:

0.5

-1.0

0.95

Used for:

Rewards

Learning rate Î± (alpha)

Discount factor Î³ (gamma)

Q-values

Computers represent floats using IEEE-754 binary format.

This means:

0.1 + 0.2 != 0.3

This is normal. RL tolerates tiny floating error.

2. Algebra Refresher

Python follows standard algebra rules.

Operators
Meaning	Symbol
Add	+
Subtract	-
Multiply	*
Divide	/
Power	**
Order of Operations (PEMDAS)
result = 2 + 3 * 4

= 14
Multiplication happens before addition.

Parentheses override:

result = (2 + 3) * 4

= 20

This matters in update equations.

3. Functions

A function maps inputs to outputs.

Mathematically:

ğ‘“
(
ğ‘¥
)
=
ğ‘¥
2
f(x)=x
2

In Python:

def f(x):
    return x**2

Reinforcement learning repeatedly applies update functions.

4. Sets and States

A state space is a set of all possible states.

In a 5Ã—5 grid:

ğ‘†
=
{
(
0
,
0
)
,
(
0
,
1
)
,
.
.
.
,
(
4
,
4
)
}
S={(0,0),(0,1),...,(4,4)}

Total states:

âˆ£
ğ‘†
âˆ£
=
ğ‘Ÿ
ğ‘œ
ğ‘¤
ğ‘ 
Ã—
ğ‘
ğ‘œ
ğ‘™
ğ‘ 
âˆ£Sâˆ£=rowsÃ—cols

If rows = 5 and cols = 5:

25
 states
25 states

Each state can have multiple actions.

5. Vectors

A vector is an ordered list of numbers.

Example:

[
1.0
,
0.5
,
âˆ’
0.2
,
3.1
]
[1.0,0.5,âˆ’0.2,3.1]

In this project:

Each state stores a vector:

ğ‘„
(
ğ‘ 
)
=
[
ğ‘„
(
ğ‘ 
,
ğ¹
ğ‘‚
ğ‘…
ğ‘Š
ğ´
ğ‘…
ğ·
)
,
ğ‘„
(
ğ‘ 
,
ğµ
ğ´
ğ¶
ğ¾
ğ‘Š
ğ´
ğ‘…
ğ·
)
,
ğ‘„
(
ğ‘ 
,
ğ‘…
ğ¼
ğº
ğ»
ğ‘‡
)
,
ğ‘„
(
ğ‘ 
,
ğ¿
ğ¸
ğ¹
ğ‘‡
)
]
Q(s)=[Q(s,FORWARD),Q(s,BACKWARD),Q(s,RIGHT),Q(s,LEFT)]

This is a value vector per state.

6. Maximum Function

We use:

max
â¡
(
ğ‘¥
1
,
ğ‘¥
2
,
.
.
.
,
ğ‘¥
ğ‘›
)
max(x
1
	â€‹

,x
2
	â€‹

,...,x
n
	â€‹

)

This selects the largest number.

In RL:

max
â¡
ğ‘
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
)
a
max
	â€‹

Q(s
â€²
,a)

Means:

From the next state, what is the best action value available?

This drives greedy learning.

7. Expected Value (Core Idea)

Expected value means:

Long-term average outcome.

If a reward is random:

ğ¸
[
ğ‘…
]
=
âˆ‘
ğ‘
(
ğ‘Ÿ
)
â‹…
ğ‘Ÿ
E[R]=âˆ‘p(r)â‹…r

In Q-learning, Q(s,a) approximates:

ğ¸
[
future total reward
]
E[future total reward]

The Q-table estimates expectation through repeated updates.

8. Discount Factor Î³ (gamma)

Gamma controls how much future rewards matter.

0
â‰¤
ğ›¾
â‰¤
1
0â‰¤Î³â‰¤1

If:

Î³ = 0 â†’ agent only cares about immediate reward

Î³ = 1 â†’ agent values future equally

Example:

Reward = 0
Future best value = 10
Î³ = 0.9

0
+
0.9
Ã—
10
=
9
0+0.9Ã—10=9

Future is discounted slightly.

9. Learning Rate Î± (alpha)

Alpha controls update speed.

0
<
ğ›¼
â‰¤
1
0<Î±â‰¤1

If:

Î± = 1 â†’ overwrite old value completely

Î± small â†’ slow, stable learning

Alpha performs weighted averaging.

10. The Q-Learning Update Rule

The central equation:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
ğ‘„
(
ğ‘ 
,
ğ‘
)
+
ğ›¼
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
)
âˆ’
ğ‘„
(
ğ‘ 
,
ğ‘
)
]
Q(s,a)â†Q(s,a)+Î±[r+Î³
a
max
	â€‹

Q(s
â€²
,a)âˆ’Q(s,a)]

Break it down:

Step 1: Compute target
ğ‘¡
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘’
ğ‘¡
=
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘„
(
ğ‘ 
â€²
)
target=r+Î³maxQ(s
â€²
)
Step 2: Compute error
ğ‘’
ğ‘Ÿ
ğ‘Ÿ
ğ‘œ
ğ‘Ÿ
=
ğ‘¡
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘’
ğ‘¡
âˆ’
ğ‘
ğ‘¢
ğ‘Ÿ
ğ‘Ÿ
ğ‘’
ğ‘›
ğ‘¡
_
ğ‘„
error=targetâˆ’current_Q
Step 3: Apply fraction of error
ğ‘›
ğ‘’
ğ‘¤
_
ğ‘„
=
ğ‘
ğ‘¢
ğ‘Ÿ
ğ‘Ÿ
ğ‘’
ğ‘›
ğ‘¡
_
ğ‘„
+
ğ›¼
Ã—
ğ‘’
ğ‘Ÿ
ğ‘Ÿ
ğ‘œ
ğ‘Ÿ
new_Q=current_Q+Î±Ã—error

This is incremental correction.

Nothing more.

11. Bellman Equation (Conceptual Form)

The Bellman optimality equation:

ğ‘„
âˆ—
(
ğ‘ 
,
ğ‘
)
=
ğ¸
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
âˆ—
(
ğ‘ 
â€²
,
ğ‘
â€²
)
]
Q
âˆ—
(s,a)=E[r+Î³
a
â€²
max
	â€‹

Q
âˆ—
(s
â€²
,a
â€²
)]

Q-learning approximates this iteratively.

You are solving a recursive fixed-point equation through sampling.

12. Markov Property

A system is Markov if:

ğ‘ƒ
(
ğ‘ 
ğ‘¡
+
1
âˆ£
ğ‘ 
ğ‘¡
)
P(s
t+1
	â€‹

âˆ£s
t
	â€‹

)

depends only on the current state.

Not the full history.

Grid world satisfies this.

Thatâ€™s why Q-learning works.

13. Convergence

Q-learning converges if:

All state-action pairs are explored

Learning rate decreases or is small

Rewards are bounded

Over time:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†’
ğ‘„
âˆ—
(
ğ‘ 
,
ğ‘
)
Q(s,a)â†’Q
âˆ—
(s,a)

The optimal value function.

14. Geometry of the Value Landscape

The Q-table forms a discrete value surface.

Each grid cell stores:

ğ‘‰
(
ğ‘ 
)
=
max
â¡
ğ‘
ğ‘„
(
ğ‘ 
,
ğ‘
)
V(s)=
a
max
	â€‹

Q(s,a)

This produces a gradient toward the goal.

The agent climbs that gradient.

15. Why Negative Step Reward Works

Each step gives:

ğ‘Ÿ
=
âˆ’
1
r=âˆ’1

Goal gives:

ğ‘Ÿ
=
0
r=0

Total reward equals:

âˆ’
number of steps
âˆ’number of steps

Thus:

Maximizing reward = minimizing path length.

Clean. Efficient. No artificial shaping.

16. Probability Basics

Exploration rate Îµ:

0
â‰¤
ğœ€
â‰¤
1
0â‰¤Îµâ‰¤1

If Îµ = 0.1:

10% random actions
90% greedy actions

This balances:

Exploration

Exploitation

17. Linear Algebra (What You Actually Need)

For this project, you only need:

Vectors

Maximum selection

Scalar multiplication

Weighted averaging

No matrices.
No eigenvalues.
No calculus.

18. Big Picture

Reinforcement learning in this repo is:

Discrete

Tabular

Iterative

Sample-based

Converging toward optimal value estimates

Mathematically, it is:

Repeated application of a contraction mapping toward a fixed point.

But practically:

Correct small prediction errors over time.

19. What To Study Next

If you want stronger foundations:

Probability theory basics

Expected value

Dynamic programming

Bellman equations

Markov Decision Processes