Markov Decision Processes (MDP) â€” The Mathematical Framework Behind This Project

This document explains the formal structure underlying the grid world and Q-learning agent.

Reinforcement learning is not arbitrary experimentation.
It is grounded in the theory of Markov Decision Processes.

1. What Is a Markov Decision Process?

An MDP is defined as a 5-tuple:

(
ğ‘†
,
ğ´
,
ğ‘ƒ
,
ğ‘…
,
ğ›¾
)
(S,A,P,R,Î³)

Where:

S = Set of states

A = Set of actions

P = Transition probability function

R = Reward function

Î³ (gamma) = Discount factor

If you understand those five components, you understand the system.

2. The Markov Property

A system is Markov if:

ğ‘ƒ
(
ğ‘ 
ğ‘¡
+
1
âˆ£
ğ‘ 
ğ‘¡
,
ğ‘
ğ‘¡
)
P(s
t+1
	â€‹

âˆ£s
t
	â€‹

,a
t
	â€‹

)

depends only on:

Current state

Current action

Not the entire history.

This means:

The present fully determines the future.

Grid world satisfies this property.

Your next position depends only on:

Where you are

Which action you take

Nothing else matters.

3. State Space (S)

In this project:

A state is:

ğ‘ 
=
(
ğ‘Ÿ
ğ‘œ
ğ‘¤
,
ğ‘
ğ‘œ
ğ‘™
)
s=(row,col)

For a 5Ã—5 grid:

âˆ£
ğ‘†
âˆ£
=
25
âˆ£Sâˆ£=25

Each coordinate pair represents a unique state.

The state space is finite and discrete.

4. Action Space (A)

Actions are discrete:

FORWARD

BACKWARD

RIGHT

LEFT

So:

âˆ£
ğ´
âˆ£
=
4
âˆ£Aâˆ£=4

Every state has the same action set.

This is a stationary action space.

5. Transition Function (P)

The transition function:

ğ‘ƒ
(
ğ‘ 
â€²
âˆ£
ğ‘ 
,
ğ‘
)
P(s
â€²
âˆ£s,a)

Defines the probability of moving from state 
ğ‘ 
s to 
ğ‘ 
â€²
s
â€²
 given action 
ğ‘
a.

In this grid world:

Transitions are deterministic.

Meaning:

ğ‘ƒ
(
ğ‘ 
â€²
âˆ£
ğ‘ 
,
ğ‘
)
=
1
P(s
â€²
âˆ£s,a)=1

for exactly one next state.

There is no randomness in movement.

6. Reward Function (R)

Reward function:

ğ‘…
(
ğ‘ 
,
ğ‘
,
ğ‘ 
â€²
)
R(s,a,s
â€²
)

Defines immediate reward received after transition.

In this project:

Every step â†’ -1

Reaching goal â†’ 0

This creates a pressure toward shortest paths.

Reward structure defines behavior.

7. Discount Factor (Î³)
0
â‰¤
ğ›¾
â‰¤
1
0â‰¤Î³â‰¤1

Gamma determines how much future reward matters.

If:

Î³ = 0 â†’ only immediate reward matters

Î³ close to 1 â†’ future rewards matter strongly

In this grid:

ğ›¾
=
0.9
Î³=0.9

This encourages long-term planning.

8. Policy (Ï€)

A policy is a function:

ğœ‹
(
ğ‘ 
)
â†’
ğ‘
Ï€(s)â†’a

It tells the agent what action to take in each state.

Two types:

Deterministic policy

Stochastic policy

Epsilon-greedy is stochastic.

9. Value Functions

There are two primary value functions.

State-Value Function
ğ‘‰
ğœ‹
(
ğ‘ 
)
V
Ï€
(s)

Expected return from state 
ğ‘ 
s following policy 
ğœ‹
Ï€.

Action-Value Function (Q-function)
ğ‘„
ğœ‹
(
ğ‘ 
,
ğ‘
)
Q
Ï€
(s,a)

Expected return from taking action 
ğ‘
a in state 
ğ‘ 
s.

This project learns:

ğ‘„
(
ğ‘ 
,
ğ‘
)
Q(s,a)

The Q-table approximates optimal action values.

10. Return (G)

Return is total discounted reward:

ğº
ğ‘¡
=
ğ‘Ÿ
ğ‘¡
+
1
+
ğ›¾
ğ‘Ÿ
ğ‘¡
+
2
+
ğ›¾
2
ğ‘Ÿ
ğ‘¡
+
3
+
.
.
.
G
t
	â€‹

=r
t+1
	â€‹

+Î³r
t+2
	â€‹

+Î³
2
r
t+3
	â€‹

+...

Q-learning estimates expected return.

11. Optimal Policy

An optimal policy satisfies:

ğœ‹
âˆ—
(
ğ‘ 
)
=
ğ‘
ğ‘Ÿ
ğ‘”
ğ‘š
ğ‘
ğ‘¥
ğ‘
ğ‘„
âˆ—
(
ğ‘ 
,
ğ‘
)
Ï€
âˆ—
(s)=argmax
a
	â€‹

Q
âˆ—
(s,a)

It chooses the action with the highest expected value.

Greedy selection extracts this policy from the Q-table.

12. Bellman Optimality Equation

The optimal Q-function satisfies:

ğ‘„
âˆ—
(
ğ‘ 
,
ğ‘
)
=
ğ¸
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
âˆ—
(
ğ‘ 
â€²
,
ğ‘
â€²
)
]
Q
âˆ—
(s,a)=E[r+Î³
a
â€²
max
	â€‹

Q
âˆ—
(s
â€²
,a
â€²
)]

This is a recursive definition.

Q-learning iteratively approximates this fixed point.

13. Why This Converges

Q-learning converges if:

Every state-action pair is visited infinitely often

Learning rate is appropriate

Rewards are bounded

Over time:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†’
ğ‘„
âˆ—
(
ğ‘ 
,
ğ‘
)
Q(s,a)â†’Q
âˆ—
(s,a)
14. Deterministic vs Stochastic MDP

This grid world is:

Deterministic transitions

Deterministic rewards

Finite state space

Fully observable

This is the simplest valid MDP.

More complex systems add:

Noise

Partial observability

Continuous state spaces

But the structure remains the same.

15. Relationship to Dynamic Programming

If full transition model is known:

You can solve MDP using:

Value Iteration

Policy Iteration

Q-learning differs because:

It does not require the transition model.

It learns through interaction.

16. Why MDP Matters

Without MDP structure:

No convergence guarantee

No theoretical grounding

No defined objective

MDP provides:

Formal objective

Optimality definition

Convergence theory

This is why RL works.

17. In This Repository

Your system is:

Finite MDP

Tabular solution

Off-policy learning

Model-free

Value-based

It is the foundational RL configuration.

Master this before moving to:

Function approximation

Deep Q Networks

Policy gradients

18. Conceptual Summary

An MDP is:

A structured decision system

With states

With actions

With transition rules

With rewards

And future discounting

Reinforcement learning is:

The process of discovering the optimal policy inside that structure.